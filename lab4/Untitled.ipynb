{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_path = '.data/cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_1 = unpickle(cifar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'training batch 1 of 5', array([[ 59,  43,  50, ..., 140,  84,  72],\n",
       "        [154, 126, 105, ..., 139, 142, 144],\n",
       "        [255, 253, 253, ...,  83,  83,  84],\n",
       "        ...,\n",
       "        [ 71,  60,  74, ...,  68,  69,  68],\n",
       "        [250, 254, 211, ..., 215, 255, 254],\n",
       "        [ 62,  61,  60, ..., 130, 130, 131]], dtype=uint8))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[b'batch_label'],data_1[b'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), # size of 32*32, padding 4 px\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=data_path, train=True, download=False, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=data_path, train=False, download=False, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res net model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet([2, 2, 2, 2]) # res18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "lr = 0.1\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 391 Loss: 2.805 | Acc: 14.844% (19/128)\n",
      "1 391 Loss: 3.090 | Acc: 15.625% (40/256)\n",
      "2 391 Loss: 3.424 | Acc: 15.365% (59/384)\n",
      "3 391 Loss: 3.842 | Acc: 15.625% (80/512)\n",
      "4 391 Loss: 4.140 | Acc: 13.906% (89/640)\n",
      "5 391 Loss: 4.702 | Acc: 13.411% (103/768)\n",
      "6 391 Loss: 4.913 | Acc: 13.504% (121/896)\n",
      "7 391 Loss: 4.864 | Acc: 12.988% (133/1024)\n",
      "8 391 Loss: 4.831 | Acc: 12.413% (143/1152)\n",
      "9 391 Loss: 4.895 | Acc: 12.188% (156/1280)\n",
      "10 391 Loss: 4.834 | Acc: 11.719% (165/1408)\n",
      "11 391 Loss: 4.874 | Acc: 11.654% (179/1536)\n",
      "12 391 Loss: 4.770 | Acc: 11.418% (190/1664)\n",
      "13 391 Loss: 4.920 | Acc: 11.272% (202/1792)\n",
      "14 391 Loss: 4.863 | Acc: 11.406% (219/1920)\n",
      "15 391 Loss: 4.954 | Acc: 11.279% (231/2048)\n",
      "16 391 Loss: 4.909 | Acc: 11.213% (244/2176)\n",
      "17 391 Loss: 4.829 | Acc: 11.328% (261/2304)\n",
      "18 391 Loss: 4.710 | Acc: 11.883% (289/2432)\n",
      "19 391 Loss: 4.613 | Acc: 12.031% (308/2560)\n",
      "20 391 Loss: 4.516 | Acc: 11.979% (322/2688)\n",
      "21 391 Loss: 4.428 | Acc: 11.896% (335/2816)\n",
      "22 391 Loss: 4.351 | Acc: 11.821% (348/2944)\n",
      "23 391 Loss: 4.269 | Acc: 11.654% (358/3072)\n",
      "24 391 Loss: 4.248 | Acc: 11.500% (368/3200)\n",
      "25 391 Loss: 4.184 | Acc: 11.508% (383/3328)\n",
      "26 391 Loss: 4.144 | Acc: 11.603% (401/3456)\n",
      "27 391 Loss: 4.075 | Acc: 11.858% (425/3584)\n",
      "28 391 Loss: 4.017 | Acc: 11.800% (438/3712)\n",
      "29 391 Loss: 3.961 | Acc: 11.927% (458/3840)\n",
      "30 391 Loss: 3.920 | Acc: 12.046% (478/3968)\n",
      "31 391 Loss: 3.871 | Acc: 12.109% (496/4096)\n",
      "32 391 Loss: 3.822 | Acc: 12.003% (507/4224)\n",
      "33 391 Loss: 3.795 | Acc: 12.017% (523/4352)\n",
      "34 391 Loss: 3.752 | Acc: 11.920% (534/4480)\n",
      "35 391 Loss: 3.707 | Acc: 12.261% (565/4608)\n",
      "36 391 Loss: 3.670 | Acc: 12.268% (581/4736)\n",
      "37 391 Loss: 3.633 | Acc: 12.500% (608/4864)\n",
      "38 391 Loss: 3.597 | Acc: 12.440% (621/4992)\n",
      "39 391 Loss: 3.564 | Acc: 12.500% (640/5120)\n",
      "40 391 Loss: 3.545 | Acc: 12.748% (669/5248)\n",
      "41 391 Loss: 3.514 | Acc: 12.909% (694/5376)\n",
      "42 391 Loss: 3.497 | Acc: 12.936% (712/5504)\n",
      "43 391 Loss: 3.467 | Acc: 12.997% (732/5632)\n",
      "44 391 Loss: 3.437 | Acc: 13.160% (758/5760)\n",
      "45 391 Loss: 3.411 | Acc: 13.332% (785/5888)\n",
      "46 391 Loss: 3.383 | Acc: 13.381% (805/6016)\n",
      "47 391 Loss: 3.356 | Acc: 13.525% (831/6144)\n",
      "48 391 Loss: 3.333 | Acc: 13.568% (851/6272)\n",
      "49 391 Loss: 3.309 | Acc: 13.797% (883/6400)\n",
      "50 391 Loss: 3.286 | Acc: 13.771% (899/6528)\n",
      "51 391 Loss: 3.263 | Acc: 13.912% (926/6656)\n",
      "52 391 Loss: 3.242 | Acc: 13.959% (947/6784)\n",
      "53 391 Loss: 3.243 | Acc: 14.077% (973/6912)\n",
      "54 391 Loss: 3.221 | Acc: 14.219% (1001/7040)\n",
      "55 391 Loss: 3.202 | Acc: 14.202% (1018/7168)\n",
      "56 391 Loss: 3.185 | Acc: 14.309% (1044/7296)\n",
      "57 391 Loss: 3.169 | Acc: 14.426% (1071/7424)\n",
      "58 391 Loss: 3.154 | Acc: 14.473% (1093/7552)\n",
      "59 391 Loss: 3.140 | Acc: 14.492% (1113/7680)\n",
      "60 391 Loss: 3.124 | Acc: 14.549% (1136/7808)\n",
      "61 391 Loss: 3.107 | Acc: 14.579% (1157/7936)\n",
      "62 391 Loss: 3.101 | Acc: 14.633% (1180/8064)\n",
      "63 391 Loss: 3.098 | Acc: 14.636% (1199/8192)\n",
      "64 391 Loss: 3.083 | Acc: 14.700% (1223/8320)\n",
      "65 391 Loss: 3.068 | Acc: 14.856% (1255/8448)\n",
      "66 391 Loss: 3.054 | Acc: 14.925% (1280/8576)\n",
      "67 391 Loss: 3.040 | Acc: 15.005% (1306/8704)\n",
      "68 391 Loss: 3.028 | Acc: 15.093% (1333/8832)\n",
      "69 391 Loss: 3.015 | Acc: 15.190% (1361/8960)\n",
      "70 391 Loss: 3.003 | Acc: 15.262% (1387/9088)\n",
      "71 391 Loss: 2.990 | Acc: 15.321% (1412/9216)\n",
      "72 391 Loss: 2.978 | Acc: 15.432% (1442/9344)\n",
      "73 391 Loss: 2.967 | Acc: 15.456% (1464/9472)\n",
      "74 391 Loss: 2.956 | Acc: 15.479% (1486/9600)\n",
      "75 391 Loss: 2.945 | Acc: 15.491% (1507/9728)\n",
      "76 391 Loss: 2.935 | Acc: 15.503% (1528/9856)\n",
      "77 391 Loss: 2.927 | Acc: 15.575% (1555/9984)\n",
      "78 391 Loss: 2.917 | Acc: 15.556% (1573/10112)\n",
      "79 391 Loss: 2.908 | Acc: 15.645% (1602/10240)\n",
      "80 391 Loss: 2.903 | Acc: 15.712% (1629/10368)\n",
      "81 391 Loss: 2.893 | Acc: 15.701% (1648/10496)\n",
      "82 391 Loss: 2.883 | Acc: 15.813% (1680/10624)\n",
      "83 391 Loss: 2.873 | Acc: 15.848% (1704/10752)\n",
      "84 391 Loss: 2.864 | Acc: 15.901% (1730/10880)\n",
      "85 391 Loss: 2.853 | Acc: 16.052% (1767/11008)\n",
      "86 391 Loss: 2.845 | Acc: 16.065% (1789/11136)\n",
      "87 391 Loss: 2.836 | Acc: 16.158% (1820/11264)\n",
      "88 391 Loss: 2.828 | Acc: 16.222% (1848/11392)\n",
      "89 391 Loss: 2.820 | Acc: 16.328% (1881/11520)\n",
      "90 391 Loss: 2.811 | Acc: 16.398% (1910/11648)\n",
      "91 391 Loss: 2.803 | Acc: 16.423% (1934/11776)\n",
      "92 391 Loss: 2.795 | Acc: 16.473% (1961/11904)\n",
      "93 391 Loss: 2.788 | Acc: 16.548% (1991/12032)\n",
      "94 391 Loss: 2.780 | Acc: 16.571% (2015/12160)\n",
      "95 391 Loss: 2.773 | Acc: 16.602% (2040/12288)\n",
      "96 391 Loss: 2.765 | Acc: 16.640% (2066/12416)\n",
      "97 391 Loss: 2.758 | Acc: 16.693% (2094/12544)\n",
      "98 391 Loss: 2.751 | Acc: 16.761% (2124/12672)\n",
      "99 391 Loss: 2.744 | Acc: 16.766% (2146/12800)\n",
      "100 391 Loss: 2.738 | Acc: 16.770% (2168/12928)\n",
      "101 391 Loss: 2.732 | Acc: 16.805% (2194/13056)\n",
      "102 391 Loss: 2.725 | Acc: 16.869% (2224/13184)\n",
      "103 391 Loss: 2.719 | Acc: 16.932% (2254/13312)\n",
      "104 391 Loss: 2.711 | Acc: 17.046% (2291/13440)\n",
      "105 391 Loss: 2.705 | Acc: 17.055% (2314/13568)\n",
      "106 391 Loss: 2.699 | Acc: 17.166% (2351/13696)\n",
      "107 391 Loss: 2.693 | Acc: 17.238% (2383/13824)\n",
      "108 391 Loss: 2.688 | Acc: 17.274% (2410/13952)\n",
      "109 391 Loss: 2.683 | Acc: 17.322% (2439/14080)\n",
      "110 391 Loss: 2.678 | Acc: 17.385% (2470/14208)\n",
      "111 391 Loss: 2.673 | Acc: 17.425% (2498/14336)\n",
      "112 391 Loss: 2.668 | Acc: 17.464% (2526/14464)\n",
      "113 391 Loss: 2.662 | Acc: 17.503% (2554/14592)\n",
      "114 391 Loss: 2.657 | Acc: 17.554% (2584/14720)\n",
      "115 391 Loss: 2.653 | Acc: 17.592% (2612/14848)\n",
      "116 391 Loss: 2.650 | Acc: 17.608% (2637/14976)\n",
      "117 391 Loss: 2.645 | Acc: 17.691% (2672/15104)\n",
      "118 391 Loss: 2.640 | Acc: 17.765% (2706/15232)\n",
      "119 391 Loss: 2.634 | Acc: 17.878% (2746/15360)\n",
      "120 391 Loss: 2.628 | Acc: 17.917% (2775/15488)\n",
      "121 391 Loss: 2.623 | Acc: 17.988% (2809/15616)\n",
      "122 391 Loss: 2.618 | Acc: 18.045% (2841/15744)\n",
      "123 391 Loss: 2.613 | Acc: 18.145% (2880/15872)\n",
      "124 391 Loss: 2.608 | Acc: 18.194% (2911/16000)\n",
      "125 391 Loss: 2.604 | Acc: 18.297% (2951/16128)\n",
      "126 391 Loss: 2.600 | Acc: 18.289% (2973/16256)\n",
      "127 391 Loss: 2.596 | Acc: 18.335% (3004/16384)\n",
      "128 391 Loss: 2.596 | Acc: 18.356% (3031/16512)\n",
      "129 391 Loss: 2.592 | Acc: 18.413% (3064/16640)\n",
      "130 391 Loss: 2.588 | Acc: 18.452% (3094/16768)\n",
      "131 391 Loss: 2.584 | Acc: 18.478% (3122/16896)\n",
      "132 391 Loss: 2.580 | Acc: 18.497% (3149/17024)\n",
      "133 391 Loss: 2.575 | Acc: 18.493% (3172/17152)\n",
      "134 391 Loss: 2.572 | Acc: 18.524% (3201/17280)\n",
      "135 391 Loss: 2.568 | Acc: 18.560% (3231/17408)\n",
      "136 391 Loss: 2.564 | Acc: 18.579% (3258/17536)\n",
      "137 391 Loss: 2.561 | Acc: 18.620% (3289/17664)\n",
      "138 391 Loss: 2.556 | Acc: 18.694% (3326/17792)\n",
      "139 391 Loss: 2.553 | Acc: 18.750% (3360/17920)\n",
      "140 391 Loss: 2.549 | Acc: 18.816% (3396/18048)\n",
      "141 391 Loss: 2.546 | Acc: 18.855% (3427/18176)\n",
      "142 391 Loss: 2.542 | Acc: 18.925% (3464/18304)\n",
      "143 391 Loss: 2.538 | Acc: 19.005% (3503/18432)\n",
      "144 391 Loss: 2.534 | Acc: 19.052% (3536/18560)\n",
      "145 391 Loss: 2.530 | Acc: 19.098% (3569/18688)\n",
      "146 391 Loss: 2.526 | Acc: 19.165% (3606/18816)\n",
      "147 391 Loss: 2.523 | Acc: 19.193% (3636/18944)\n",
      "148 391 Loss: 2.520 | Acc: 19.201% (3662/19072)\n",
      "149 391 Loss: 2.517 | Acc: 19.245% (3695/19200)\n",
      "150 391 Loss: 2.513 | Acc: 19.236% (3718/19328)\n",
      "151 391 Loss: 2.510 | Acc: 19.279% (3751/19456)\n",
      "152 391 Loss: 2.506 | Acc: 19.301% (3780/19584)\n",
      "153 391 Loss: 2.502 | Acc: 19.354% (3815/19712)\n",
      "154 391 Loss: 2.499 | Acc: 19.456% (3860/19840)\n",
      "155 391 Loss: 2.495 | Acc: 19.531% (3900/19968)\n",
      "156 391 Loss: 2.492 | Acc: 19.551% (3929/20096)\n",
      "157 391 Loss: 2.489 | Acc: 19.596% (3963/20224)\n",
      "158 391 Loss: 2.485 | Acc: 19.630% (3995/20352)\n",
      "159 391 Loss: 2.482 | Acc: 19.678% (4030/20480)\n",
      "160 391 Loss: 2.478 | Acc: 19.759% (4072/20608)\n",
      "161 391 Loss: 2.475 | Acc: 19.792% (4104/20736)\n",
      "162 391 Loss: 2.472 | Acc: 19.881% (4148/20864)\n",
      "163 391 Loss: 2.469 | Acc: 19.908% (4179/20992)\n",
      "164 391 Loss: 2.467 | Acc: 19.929% (4209/21120)\n",
      "165 391 Loss: 2.463 | Acc: 19.964% (4242/21248)\n",
      "166 391 Loss: 2.460 | Acc: 19.994% (4274/21376)\n",
      "167 391 Loss: 2.458 | Acc: 20.043% (4310/21504)\n",
      "168 391 Loss: 2.454 | Acc: 20.118% (4352/21632)\n",
      "169 391 Loss: 2.451 | Acc: 20.165% (4388/21760)\n",
      "170 391 Loss: 2.449 | Acc: 20.207% (4423/21888)\n",
      "171 391 Loss: 2.445 | Acc: 20.299% (4469/22016)\n",
      "172 391 Loss: 2.443 | Acc: 20.326% (4501/22144)\n",
      "173 391 Loss: 2.440 | Acc: 20.321% (4526/22272)\n",
      "174 391 Loss: 2.437 | Acc: 20.348% (4558/22400)\n",
      "175 391 Loss: 2.435 | Acc: 20.379% (4591/22528)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 391 Loss: 2.432 | Acc: 20.401% (4622/22656)\n",
      "177 391 Loss: 2.429 | Acc: 20.462% (4662/22784)\n",
      "178 391 Loss: 2.426 | Acc: 20.539% (4706/22912)\n",
      "179 391 Loss: 2.425 | Acc: 20.556% (4736/23040)\n",
      "180 391 Loss: 2.422 | Acc: 20.584% (4769/23168)\n",
      "181 391 Loss: 2.419 | Acc: 20.639% (4808/23296)\n",
      "182 391 Loss: 2.417 | Acc: 20.654% (4838/23424)\n",
      "183 391 Loss: 2.414 | Acc: 20.724% (4881/23552)\n",
      "184 391 Loss: 2.411 | Acc: 20.764% (4917/23680)\n",
      "185 391 Loss: 2.409 | Acc: 20.775% (4946/23808)\n",
      "186 391 Loss: 2.407 | Acc: 20.776% (4973/23936)\n",
      "187 391 Loss: 2.404 | Acc: 20.844% (5016/24064)\n",
      "188 391 Loss: 2.402 | Acc: 20.883% (5052/24192)\n",
      "189 391 Loss: 2.399 | Acc: 20.913% (5086/24320)\n",
      "190 391 Loss: 2.396 | Acc: 20.963% (5125/24448)\n",
      "191 391 Loss: 2.394 | Acc: 20.996% (5160/24576)\n",
      "192 391 Loss: 2.391 | Acc: 21.041% (5198/24704)\n",
      "193 391 Loss: 2.388 | Acc: 21.102% (5240/24832)\n",
      "194 391 Loss: 2.386 | Acc: 21.114% (5270/24960)\n",
      "195 391 Loss: 2.384 | Acc: 21.122% (5299/25088)\n",
      "196 391 Loss: 2.381 | Acc: 21.153% (5334/25216)\n",
      "197 391 Loss: 2.378 | Acc: 21.220% (5378/25344)\n",
      "198 391 Loss: 2.376 | Acc: 21.227% (5407/25472)\n",
      "199 391 Loss: 2.375 | Acc: 21.207% (5429/25600)\n",
      "200 391 Loss: 2.373 | Acc: 21.214% (5458/25728)\n",
      "201 391 Loss: 2.370 | Acc: 21.283% (5503/25856)\n",
      "202 391 Loss: 2.367 | Acc: 21.340% (5545/25984)\n",
      "203 391 Loss: 2.366 | Acc: 21.358% (5577/26112)\n",
      "204 391 Loss: 2.364 | Acc: 21.395% (5614/26240)\n",
      "205 391 Loss: 2.362 | Acc: 21.420% (5648/26368)\n",
      "206 391 Loss: 2.360 | Acc: 21.479% (5691/26496)\n",
      "207 391 Loss: 2.358 | Acc: 21.503% (5725/26624)\n",
      "208 391 Loss: 2.356 | Acc: 21.542% (5763/26752)\n",
      "209 391 Loss: 2.353 | Acc: 21.600% (5806/26880)\n",
      "210 391 Loss: 2.350 | Acc: 21.675% (5854/27008)\n",
      "211 391 Loss: 2.348 | Acc: 21.698% (5888/27136)\n",
      "212 391 Loss: 2.346 | Acc: 21.747% (5929/27264)\n",
      "213 391 Loss: 2.344 | Acc: 21.784% (5967/27392)\n",
      "214 391 Loss: 2.342 | Acc: 21.820% (6005/27520)\n",
      "215 391 Loss: 2.340 | Acc: 21.842% (6039/27648)\n",
      "216 391 Loss: 2.338 | Acc: 21.882% (6078/27776)\n",
      "217 391 Loss: 2.335 | Acc: 21.911% (6114/27904)\n",
      "218 391 Loss: 2.334 | Acc: 21.918% (6144/28032)\n",
      "219 391 Loss: 2.332 | Acc: 21.957% (6183/28160)\n",
      "220 391 Loss: 2.330 | Acc: 21.981% (6218/28288)\n",
      "221 391 Loss: 2.327 | Acc: 22.054% (6267/28416)\n",
      "222 391 Loss: 2.326 | Acc: 22.089% (6305/28544)\n",
      "223 391 Loss: 2.324 | Acc: 22.133% (6346/28672)\n",
      "224 391 Loss: 2.322 | Acc: 22.153% (6380/28800)\n",
      "225 391 Loss: 2.321 | Acc: 22.179% (6416/28928)\n",
      "226 391 Loss: 2.319 | Acc: 22.229% (6459/29056)\n",
      "227 391 Loss: 2.317 | Acc: 22.228% (6487/29184)\n",
      "228 391 Loss: 2.315 | Acc: 22.274% (6529/29312)\n",
      "229 391 Loss: 2.314 | Acc: 22.296% (6564/29440)\n",
      "230 391 Loss: 2.312 | Acc: 22.352% (6609/29568)\n",
      "231 391 Loss: 2.310 | Acc: 22.383% (6647/29696)\n",
      "232 391 Loss: 2.308 | Acc: 22.418% (6686/29824)\n",
      "233 391 Loss: 2.306 | Acc: 22.473% (6731/29952)\n",
      "234 391 Loss: 2.305 | Acc: 22.500% (6768/30080)\n",
      "235 391 Loss: 2.303 | Acc: 22.517% (6802/30208)\n",
      "236 391 Loss: 2.302 | Acc: 22.534% (6836/30336)\n",
      "237 391 Loss: 2.300 | Acc: 22.581% (6879/30464)\n",
      "238 391 Loss: 2.298 | Acc: 22.610% (6917/30592)\n",
      "239 391 Loss: 2.296 | Acc: 22.633% (6953/30720)\n",
      "240 391 Loss: 2.294 | Acc: 22.676% (6995/30848)\n",
      "241 391 Loss: 2.292 | Acc: 22.737% (7043/30976)\n",
      "242 391 Loss: 2.290 | Acc: 22.756% (7078/31104)\n",
      "243 391 Loss: 2.288 | Acc: 22.778% (7114/31232)\n",
      "244 391 Loss: 2.286 | Acc: 22.822% (7157/31360)\n",
      "245 391 Loss: 2.284 | Acc: 22.844% (7193/31488)\n",
      "246 391 Loss: 2.282 | Acc: 22.874% (7232/31616)\n",
      "247 391 Loss: 2.280 | Acc: 22.911% (7273/31744)\n",
      "248 391 Loss: 2.279 | Acc: 22.926% (7307/31872)\n",
      "249 391 Loss: 2.277 | Acc: 22.956% (7346/32000)\n",
      "250 391 Loss: 2.276 | Acc: 22.968% (7379/32128)\n",
      "251 391 Loss: 2.274 | Acc: 23.000% (7419/32256)\n",
      "252 391 Loss: 2.273 | Acc: 23.018% (7454/32384)\n",
      "253 391 Loss: 2.271 | Acc: 23.022% (7485/32512)\n",
      "254 391 Loss: 2.269 | Acc: 23.048% (7523/32640)\n",
      "255 391 Loss: 2.267 | Acc: 23.077% (7562/32768)\n",
      "256 391 Loss: 2.265 | Acc: 23.118% (7605/32896)\n",
      "257 391 Loss: 2.264 | Acc: 23.156% (7647/33024)\n",
      "258 391 Loss: 2.262 | Acc: 23.178% (7684/33152)\n",
      "259 391 Loss: 2.261 | Acc: 23.161% (7708/33280)\n",
      "260 391 Loss: 2.259 | Acc: 23.198% (7750/33408)\n",
      "261 391 Loss: 2.258 | Acc: 23.220% (7787/33536)\n",
      "262 391 Loss: 2.256 | Acc: 23.230% (7820/33664)\n",
      "263 391 Loss: 2.254 | Acc: 23.248% (7856/33792)\n",
      "264 391 Loss: 2.253 | Acc: 23.272% (7894/33920)\n",
      "265 391 Loss: 2.251 | Acc: 23.311% (7937/34048)\n",
      "266 391 Loss: 2.250 | Acc: 23.329% (7973/34176)\n",
      "267 391 Loss: 2.248 | Acc: 23.391% (8024/34304)\n",
      "268 391 Loss: 2.247 | Acc: 23.391% (8054/34432)\n",
      "269 391 Loss: 2.245 | Acc: 23.449% (8104/34560)\n",
      "270 391 Loss: 2.243 | Acc: 23.481% (8145/34688)\n",
      "271 391 Loss: 2.241 | Acc: 23.538% (8195/34816)\n",
      "272 391 Loss: 2.240 | Acc: 23.569% (8236/34944)\n",
      "273 391 Loss: 2.238 | Acc: 23.609% (8280/35072)\n",
      "274 391 Loss: 2.236 | Acc: 23.642% (8322/35200)\n",
      "275 391 Loss: 2.234 | Acc: 23.678% (8365/35328)\n",
      "276 391 Loss: 2.233 | Acc: 23.708% (8406/35456)\n",
      "277 391 Loss: 2.231 | Acc: 23.724% (8442/35584)\n",
      "278 391 Loss: 2.230 | Acc: 23.754% (8483/35712)\n",
      "279 391 Loss: 2.228 | Acc: 23.800% (8530/35840)\n",
      "280 391 Loss: 2.227 | Acc: 23.830% (8571/35968)\n",
      "281 391 Loss: 2.225 | Acc: 23.853% (8610/36096)\n",
      "282 391 Loss: 2.224 | Acc: 23.865% (8645/36224)\n",
      "283 391 Loss: 2.222 | Acc: 23.919% (8695/36352)\n",
      "284 391 Loss: 2.221 | Acc: 23.956% (8739/36480)\n",
      "285 391 Loss: 2.219 | Acc: 23.973% (8776/36608)\n",
      "286 391 Loss: 2.218 | Acc: 23.987% (8812/36736)\n",
      "287 391 Loss: 2.217 | Acc: 24.023% (8856/36864)\n",
      "288 391 Loss: 2.215 | Acc: 24.048% (8896/36992)\n",
      "289 391 Loss: 2.214 | Acc: 24.100% (8946/37120)\n",
      "290 391 Loss: 2.212 | Acc: 24.133% (8989/37248)\n",
      "291 391 Loss: 2.211 | Acc: 24.160% (9030/37376)\n",
      "292 391 Loss: 2.209 | Acc: 24.168% (9064/37504)\n",
      "293 391 Loss: 2.208 | Acc: 24.192% (9104/37632)\n",
      "294 391 Loss: 2.206 | Acc: 24.227% (9148/37760)\n",
      "295 391 Loss: 2.205 | Acc: 24.243% (9185/37888)\n",
      "296 391 Loss: 2.203 | Acc: 24.271% (9227/38016)\n",
      "297 391 Loss: 2.202 | Acc: 24.292% (9266/38144)\n",
      "298 391 Loss: 2.201 | Acc: 24.328% (9311/38272)\n",
      "299 391 Loss: 2.200 | Acc: 24.344% (9348/38400)\n",
      "300 391 Loss: 2.199 | Acc: 24.395% (9399/38528)\n",
      "301 391 Loss: 2.198 | Acc: 24.418% (9439/38656)\n",
      "302 391 Loss: 2.197 | Acc: 24.417% (9470/38784)\n",
      "303 391 Loss: 2.195 | Acc: 24.429% (9506/38912)\n",
      "304 391 Loss: 2.194 | Acc: 24.460% (9549/39040)\n",
      "305 391 Loss: 2.192 | Acc: 24.510% (9600/39168)\n",
      "306 391 Loss: 2.191 | Acc: 24.562% (9652/39296)\n",
      "307 391 Loss: 2.189 | Acc: 24.566% (9685/39424)\n",
      "308 391 Loss: 2.188 | Acc: 24.588% (9725/39552)\n",
      "309 391 Loss: 2.186 | Acc: 24.619% (9769/39680)\n",
      "310 391 Loss: 2.185 | Acc: 24.653% (9814/39808)\n",
      "311 391 Loss: 2.184 | Acc: 24.649% (9844/39936)\n",
      "312 391 Loss: 2.183 | Acc: 24.673% (9885/40064)\n",
      "313 391 Loss: 2.181 | Acc: 24.714% (9933/40192)\n",
      "314 391 Loss: 2.180 | Acc: 24.740% (9975/40320)\n",
      "315 391 Loss: 2.179 | Acc: 24.770% (10019/40448)\n",
      "316 391 Loss: 2.178 | Acc: 24.778% (10054/40576)\n",
      "317 391 Loss: 2.176 | Acc: 24.828% (10106/40704)\n",
      "318 391 Loss: 2.175 | Acc: 24.875% (10157/40832)\n",
      "319 391 Loss: 2.174 | Acc: 24.905% (10201/40960)\n",
      "320 391 Loss: 2.172 | Acc: 24.898% (10230/41088)\n",
      "321 391 Loss: 2.172 | Acc: 24.918% (10270/41216)\n",
      "322 391 Loss: 2.171 | Acc: 24.925% (10305/41344)\n",
      "323 391 Loss: 2.170 | Acc: 24.945% (10345/41472)\n",
      "324 391 Loss: 2.168 | Acc: 24.983% (10393/41600)\n",
      "325 391 Loss: 2.167 | Acc: 25.002% (10433/41728)\n",
      "326 391 Loss: 2.166 | Acc: 25.012% (10469/41856)\n",
      "327 391 Loss: 2.165 | Acc: 25.055% (10519/41984)\n",
      "328 391 Loss: 2.163 | Acc: 25.088% (10565/42112)\n",
      "329 391 Loss: 2.162 | Acc: 25.121% (10611/42240)\n",
      "330 391 Loss: 2.161 | Acc: 25.158% (10659/42368)\n",
      "331 391 Loss: 2.160 | Acc: 25.174% (10698/42496)\n",
      "332 391 Loss: 2.158 | Acc: 25.206% (10744/42624)\n",
      "333 391 Loss: 2.157 | Acc: 25.234% (10788/42752)\n",
      "334 391 Loss: 2.156 | Acc: 25.275% (10838/42880)\n",
      "335 391 Loss: 2.154 | Acc: 25.302% (10882/43008)\n",
      "336 391 Loss: 2.153 | Acc: 25.327% (10925/43136)\n",
      "337 391 Loss: 2.152 | Acc: 25.349% (10967/43264)\n",
      "338 391 Loss: 2.151 | Acc: 25.348% (10999/43392)\n",
      "339 391 Loss: 2.150 | Acc: 25.370% (11041/43520)\n",
      "340 391 Loss: 2.149 | Acc: 25.394% (11084/43648)\n",
      "341 391 Loss: 2.148 | Acc: 25.413% (11125/43776)\n",
      "342 391 Loss: 2.147 | Acc: 25.428% (11164/43904)\n",
      "343 391 Loss: 2.146 | Acc: 25.443% (11203/44032)\n",
      "344 391 Loss: 2.145 | Acc: 25.460% (11243/44160)\n",
      "345 391 Loss: 2.144 | Acc: 25.465% (11278/44288)\n",
      "346 391 Loss: 2.143 | Acc: 25.473% (11314/44416)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 391 Loss: 2.142 | Acc: 25.507% (11362/44544)\n",
      "348 391 Loss: 2.141 | Acc: 25.537% (11408/44672)\n",
      "349 391 Loss: 2.139 | Acc: 25.567% (11454/44800)\n",
      "350 391 Loss: 2.138 | Acc: 25.588% (11496/44928)\n",
      "351 391 Loss: 2.138 | Acc: 25.617% (11542/45056)\n",
      "352 391 Loss: 2.137 | Acc: 25.622% (11577/45184)\n",
      "353 391 Loss: 2.136 | Acc: 25.631% (11614/45312)\n",
      "354 391 Loss: 2.135 | Acc: 25.651% (11656/45440)\n",
      "355 391 Loss: 2.133 | Acc: 25.682% (11703/45568)\n",
      "356 391 Loss: 2.133 | Acc: 25.698% (11743/45696)\n",
      "357 391 Loss: 2.132 | Acc: 25.729% (11790/45824)\n",
      "358 391 Loss: 2.130 | Acc: 25.755% (11835/45952)\n",
      "359 391 Loss: 2.129 | Acc: 25.766% (11873/46080)\n",
      "360 391 Loss: 2.128 | Acc: 25.779% (11912/46208)\n",
      "361 391 Loss: 2.128 | Acc: 25.805% (11957/46336)\n",
      "362 391 Loss: 2.127 | Acc: 25.826% (12000/46464)\n",
      "363 391 Loss: 2.126 | Acc: 25.854% (12046/46592)\n",
      "364 391 Loss: 2.125 | Acc: 25.878% (12090/46720)\n",
      "365 391 Loss: 2.124 | Acc: 25.911% (12139/46848)\n",
      "366 391 Loss: 2.123 | Acc: 25.928% (12180/46976)\n",
      "367 391 Loss: 2.123 | Acc: 25.957% (12227/47104)\n",
      "368 391 Loss: 2.121 | Acc: 25.993% (12277/47232)\n",
      "369 391 Loss: 2.120 | Acc: 26.018% (12322/47360)\n",
      "370 391 Loss: 2.120 | Acc: 26.040% (12366/47488)\n",
      "371 391 Loss: 2.119 | Acc: 26.052% (12405/47616)\n",
      "372 391 Loss: 2.118 | Acc: 26.072% (12448/47744)\n",
      "373 391 Loss: 2.117 | Acc: 26.101% (12495/47872)\n",
      "374 391 Loss: 2.116 | Acc: 26.127% (12541/48000)\n",
      "375 391 Loss: 2.115 | Acc: 26.155% (12588/48128)\n",
      "376 391 Loss: 2.114 | Acc: 26.173% (12630/48256)\n",
      "377 391 Loss: 2.113 | Acc: 26.176% (12665/48384)\n",
      "378 391 Loss: 2.112 | Acc: 26.204% (12712/48512)\n",
      "379 391 Loss: 2.111 | Acc: 26.240% (12763/48640)\n",
      "380 391 Loss: 2.110 | Acc: 26.257% (12805/48768)\n",
      "381 391 Loss: 2.109 | Acc: 26.268% (12844/48896)\n",
      "382 391 Loss: 2.108 | Acc: 26.299% (12893/49024)\n",
      "383 391 Loss: 2.107 | Acc: 26.324% (12939/49152)\n",
      "384 391 Loss: 2.106 | Acc: 26.347% (12984/49280)\n",
      "385 391 Loss: 2.105 | Acc: 26.362% (13025/49408)\n",
      "386 391 Loss: 2.104 | Acc: 26.393% (13074/49536)\n",
      "387 391 Loss: 2.103 | Acc: 26.405% (13114/49664)\n",
      "388 391 Loss: 2.103 | Acc: 26.428% (13159/49792)\n",
      "389 391 Loss: 2.102 | Acc: 26.442% (13200/49920)\n",
      "390 391 Loss: 2.101 | Acc: 26.452% (13226/50000)\n",
      "0 100 Loss: 1.598 | Acc: 41.000% (41/100)\n",
      "1 100 Loss: 1.629 | Acc: 36.500% (73/200)\n",
      "2 100 Loss: 1.614 | Acc: 35.333% (106/300)\n",
      "3 100 Loss: 1.626 | Acc: 36.250% (145/400)\n",
      "4 100 Loss: 1.639 | Acc: 36.600% (183/500)\n",
      "5 100 Loss: 1.648 | Acc: 36.667% (220/600)\n",
      "6 100 Loss: 1.656 | Acc: 37.286% (261/700)\n",
      "7 100 Loss: 1.660 | Acc: 38.000% (304/800)\n",
      "8 100 Loss: 1.657 | Acc: 38.778% (349/900)\n",
      "9 100 Loss: 1.653 | Acc: 39.500% (395/1000)\n",
      "10 100 Loss: 1.653 | Acc: 39.545% (435/1100)\n",
      "11 100 Loss: 1.650 | Acc: 39.000% (468/1200)\n",
      "12 100 Loss: 1.658 | Acc: 38.923% (506/1300)\n",
      "13 100 Loss: 1.661 | Acc: 38.857% (544/1400)\n",
      "14 100 Loss: 1.653 | Acc: 39.533% (593/1500)\n",
      "15 100 Loss: 1.655 | Acc: 39.250% (628/1600)\n",
      "16 100 Loss: 1.649 | Acc: 39.824% (677/1700)\n",
      "17 100 Loss: 1.648 | Acc: 40.056% (721/1800)\n",
      "18 100 Loss: 1.654 | Acc: 40.053% (761/1900)\n",
      "19 100 Loss: 1.659 | Acc: 40.050% (801/2000)\n",
      "20 100 Loss: 1.662 | Acc: 39.667% (833/2100)\n",
      "21 100 Loss: 1.661 | Acc: 39.682% (873/2200)\n",
      "22 100 Loss: 1.660 | Acc: 40.087% (922/2300)\n",
      "23 100 Loss: 1.661 | Acc: 40.000% (960/2400)\n",
      "24 100 Loss: 1.660 | Acc: 40.280% (1007/2500)\n",
      "25 100 Loss: 1.668 | Acc: 39.885% (1037/2600)\n",
      "26 100 Loss: 1.668 | Acc: 40.000% (1080/2700)\n",
      "27 100 Loss: 1.664 | Acc: 40.214% (1126/2800)\n",
      "28 100 Loss: 1.667 | Acc: 40.138% (1164/2900)\n",
      "29 100 Loss: 1.667 | Acc: 39.833% (1195/3000)\n",
      "30 100 Loss: 1.662 | Acc: 39.935% (1238/3100)\n",
      "31 100 Loss: 1.660 | Acc: 40.156% (1285/3200)\n",
      "32 100 Loss: 1.660 | Acc: 40.182% (1326/3300)\n",
      "33 100 Loss: 1.660 | Acc: 40.059% (1362/3400)\n",
      "34 100 Loss: 1.662 | Acc: 39.914% (1397/3500)\n",
      "35 100 Loss: 1.661 | Acc: 39.889% (1436/3600)\n",
      "36 100 Loss: 1.661 | Acc: 39.838% (1474/3700)\n",
      "37 100 Loss: 1.660 | Acc: 39.789% (1512/3800)\n",
      "38 100 Loss: 1.660 | Acc: 39.872% (1555/3900)\n",
      "39 100 Loss: 1.660 | Acc: 39.825% (1593/4000)\n",
      "40 100 Loss: 1.660 | Acc: 39.805% (1632/4100)\n",
      "41 100 Loss: 1.660 | Acc: 39.762% (1670/4200)\n",
      "42 100 Loss: 1.661 | Acc: 39.791% (1711/4300)\n",
      "43 100 Loss: 1.660 | Acc: 39.773% (1750/4400)\n",
      "44 100 Loss: 1.660 | Acc: 39.778% (1790/4500)\n",
      "45 100 Loss: 1.660 | Acc: 39.674% (1825/4600)\n",
      "46 100 Loss: 1.658 | Acc: 39.766% (1869/4700)\n",
      "47 100 Loss: 1.658 | Acc: 39.729% (1907/4800)\n",
      "48 100 Loss: 1.656 | Acc: 39.816% (1951/4900)\n",
      "49 100 Loss: 1.654 | Acc: 39.940% (1997/5000)\n",
      "50 100 Loss: 1.654 | Acc: 39.902% (2035/5100)\n",
      "51 100 Loss: 1.653 | Acc: 40.058% (2083/5200)\n",
      "52 100 Loss: 1.655 | Acc: 39.925% (2116/5300)\n",
      "53 100 Loss: 1.657 | Acc: 39.870% (2153/5400)\n",
      "54 100 Loss: 1.658 | Acc: 39.818% (2190/5500)\n",
      "55 100 Loss: 1.658 | Acc: 39.750% (2226/5600)\n",
      "56 100 Loss: 1.660 | Acc: 39.684% (2262/5700)\n",
      "57 100 Loss: 1.657 | Acc: 39.724% (2304/5800)\n",
      "58 100 Loss: 1.658 | Acc: 39.627% (2338/5900)\n",
      "59 100 Loss: 1.659 | Acc: 39.600% (2376/6000)\n",
      "60 100 Loss: 1.659 | Acc: 39.541% (2412/6100)\n",
      "61 100 Loss: 1.659 | Acc: 39.468% (2447/6200)\n",
      "62 100 Loss: 1.660 | Acc: 39.429% (2484/6300)\n",
      "63 100 Loss: 1.659 | Acc: 39.391% (2521/6400)\n",
      "64 100 Loss: 1.660 | Acc: 39.354% (2558/6500)\n",
      "65 100 Loss: 1.660 | Acc: 39.318% (2595/6600)\n",
      "66 100 Loss: 1.661 | Acc: 39.224% (2628/6700)\n",
      "67 100 Loss: 1.661 | Acc: 39.176% (2664/6800)\n",
      "68 100 Loss: 1.662 | Acc: 39.130% (2700/6900)\n",
      "69 100 Loss: 1.663 | Acc: 39.014% (2731/7000)\n",
      "70 100 Loss: 1.665 | Acc: 38.944% (2765/7100)\n",
      "71 100 Loss: 1.665 | Acc: 38.944% (2804/7200)\n",
      "72 100 Loss: 1.664 | Acc: 39.000% (2847/7300)\n",
      "73 100 Loss: 1.663 | Acc: 39.068% (2891/7400)\n",
      "74 100 Loss: 1.663 | Acc: 39.053% (2929/7500)\n",
      "75 100 Loss: 1.663 | Acc: 39.026% (2966/7600)\n",
      "76 100 Loss: 1.661 | Acc: 39.130% (3013/7700)\n",
      "77 100 Loss: 1.660 | Acc: 39.192% (3057/7800)\n",
      "78 100 Loss: 1.658 | Acc: 39.266% (3102/7900)\n",
      "79 100 Loss: 1.659 | Acc: 39.212% (3137/8000)\n",
      "80 100 Loss: 1.658 | Acc: 39.272% (3181/8100)\n",
      "81 100 Loss: 1.658 | Acc: 39.268% (3220/8200)\n",
      "82 100 Loss: 1.658 | Acc: 39.277% (3260/8300)\n",
      "83 100 Loss: 1.658 | Acc: 39.333% (3304/8400)\n",
      "84 100 Loss: 1.659 | Acc: 39.176% (3330/8500)\n",
      "85 100 Loss: 1.659 | Acc: 39.279% (3378/8600)\n",
      "86 100 Loss: 1.660 | Acc: 39.230% (3413/8700)\n",
      "87 100 Loss: 1.660 | Acc: 39.261% (3455/8800)\n",
      "88 100 Loss: 1.661 | Acc: 39.247% (3493/8900)\n",
      "89 100 Loss: 1.662 | Acc: 39.167% (3525/9000)\n",
      "90 100 Loss: 1.661 | Acc: 39.110% (3559/9100)\n",
      "91 100 Loss: 1.661 | Acc: 39.130% (3600/9200)\n",
      "92 100 Loss: 1.661 | Acc: 39.226% (3648/9300)\n",
      "93 100 Loss: 1.661 | Acc: 39.245% (3689/9400)\n",
      "94 100 Loss: 1.661 | Acc: 39.263% (3730/9500)\n",
      "95 100 Loss: 1.660 | Acc: 39.219% (3765/9600)\n",
      "96 100 Loss: 1.660 | Acc: 39.206% (3803/9700)\n",
      "97 100 Loss: 1.661 | Acc: 39.173% (3839/9800)\n",
      "98 100 Loss: 1.662 | Acc: 39.141% (3875/9900)\n",
      "99 100 Loss: 1.661 | Acc: 39.160% (3916/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 391 Loss: 1.735 | Acc: 33.594% (43/128)\n",
      "1 391 Loss: 1.757 | Acc: 35.156% (90/256)\n",
      "2 391 Loss: 1.765 | Acc: 34.115% (131/384)\n",
      "3 391 Loss: 1.750 | Acc: 34.961% (179/512)\n",
      "4 391 Loss: 1.759 | Acc: 33.594% (215/640)\n",
      "5 391 Loss: 1.748 | Acc: 34.505% (265/768)\n",
      "6 391 Loss: 1.740 | Acc: 35.491% (318/896)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f940896280be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-d016073a11f8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, start_epoch+200):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
